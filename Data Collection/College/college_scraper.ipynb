{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base URLs for the different stat categories\n",
    "base_urls = {\n",
    "    'batting_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=bat&year={}&group=College',\n",
    "    'pitching_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=pitch&year={}&group=College',\n",
    "    'fielding_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=field&year={}&group=College',\n",
    "    'team_batting': 'https://www.baseball-reference.com/register/team.cgi?request=1&type=bat&year={}&group=College',\n",
    "    'team_pitching': 'https://www.baseball-reference.com/register/team.cgi?request=1&type=pitch&year={}&group=College',\n",
    "    'drafted_players': 'https://www.baseball-reference.com/draft/?query_type=year_round&year_ID={}&draft_round=1&draft_type=college'\n",
    "}\n",
    "\n",
    "# Function to scrape data from a specific URL and save it as a CSV file\n",
    "def scrape_data(url, filename):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the table with the class 'stats_table'\n",
    "    table = soup.find('table', {'class': 'stats_table'})\n",
    "    if table:\n",
    "        # Extract headers\n",
    "        headers = [th.text.strip() for th in table.find_all('th')]\n",
    "        \n",
    "        # Debugging: Print the number of columns in headers\n",
    "        print(f'Number of columns in headers: {len(headers)}')\n",
    "\n",
    "        # Extract rows\n",
    "        rows = []\n",
    "        for tr in table.find_all('tr')[1:]:  # Skip the header row\n",
    "            cols = tr.find_all('td')\n",
    "            if cols:\n",
    "                rows.append([td.text.strip() for td in cols])\n",
    "\n",
    "        # Debugging: Print the number of columns in the first row\n",
    "        if rows:\n",
    "            print(f'Number of columns in first row: {len(rows[0])}')\n",
    "\n",
    "        # Adjust the number of columns if there's a mismatch\n",
    "        if len(headers[1:]) != len(rows[0]):  # headers[1:] skips the empty first header\n",
    "            headers = headers[:1] + headers[1:len(rows[0])+1]  # Match header length to row length\n",
    "\n",
    "        # Create a DataFrame and save it as a CSV file\n",
    "        df = pd.DataFrame(rows, columns=headers[1:])  # Avoid first empty header\n",
    "        df.to_csv(f'{filename}.csv', index=False)\n",
    "        print(f'{filename}.csv saved.')\n",
    "    else:\n",
    "        print(f'No table found at {url}')\n",
    "\n",
    "# Function to scrape data for a range of years\n",
    "def scrape_data_for_years(start_year, end_year):\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        print(f'Scraping data for {year}...')\n",
    "\n",
    "        # Scrape data for each category\n",
    "        scrape_data(base_urls['batting_leaders'].format(year), f'college_batting_leaders_{year}')\n",
    "        scrape_data(base_urls['pitching_leaders'].format(year), f'college_pitching_leaders_{year}')\n",
    "        scrape_data(base_urls['fielding_leaders'].format(year), f'college_fielding_leaders_{year}')\n",
    "        scrape_data(base_urls['team_batting'].format(year), f'college_team_batting_{year}')\n",
    "        scrape_data(base_urls['team_pitching'].format(year), f'college_team_pitching_{year}')\n",
    "        scrape_data(base_urls['drafted_players'].format(year), f'college_drafted_players_{year}')\n",
    "\n",
    "# Specify the range of years (e.g., from 2015 to 2024)\n",
    "scrape_data_for_years(2015, 2024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different version of the scraper above that ignores the 2020 season entirely due to issues with collecting data after that year \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Base URLs\n",
    "base_urls = {\n",
    "    'batting_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=bat&year={}&group=College',\n",
    "    'pitching_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=pitch&year={}&group=College',\n",
    "    'fielding_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=field&year={}&group=College',\n",
    "    'team_batting': 'https://www.baseball-reference.com/register/team.cgi?request=1&type=bat&year={}&group=College',\n",
    "    'team_pitching': 'https://www.baseball-reference.com/register/team.cgi?request=1&type=pitch&year={}&group=College',\n",
    "    'drafted_players': 'https://www.baseball-reference.com/draft/?query_type=year_round&year_ID={}&draft_round=1&draft_type=college'\n",
    "}\n",
    "\n",
    "def scrape_data(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        \n",
    "        if not table:\n",
    "            print(f'No table found at {url}')\n",
    "            return\n",
    "        \n",
    "        headers = [th.get_text() for th in table.find_all('th')]\n",
    "        rows = []\n",
    "        for row in table.find_all('tr'):\n",
    "            rows.append([td.get_text() for td in row.find_all('td')])\n",
    "        \n",
    "        # Check if the headers and data match up\n",
    "        if len(headers) > 1 and len(headers) != len(rows[0]):\n",
    "            print(f\"Number of columns in headers: {len(headers)}\")\n",
    "            print(f\"Number of columns in first row: {len(rows[0])}\")\n",
    "        \n",
    "        # Convert to DataFrame and save as CSV\n",
    "        df = pd.DataFrame(rows, columns=headers)\n",
    "        df.to_csv(f'{filename}.csv', index=False)\n",
    "        print(f'{filename}.csv saved.')\n",
    "        \n",
    "    except requests.HTTPError as http_err:\n",
    "        print(f'HTTP error occurred: {http_err}')\n",
    "    except Exception as err:\n",
    "        print(f'An error occurred: {err}')\n",
    "\n",
    "def scrape_data_for_years(start_year, end_year):\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        # Skip the 2020 season entirely\n",
    "        if year == 2020:\n",
    "            print(f'Skipping the 2020 season...')\n",
    "            continue\n",
    "        \n",
    "        print(f'Scraping data for {year}...')\n",
    "        \n",
    "        # Scrape each category\n",
    "        scrape_data(base_urls['batting_leaders'].format(year), f'college_batting_leaders_{year}')\n",
    "        scrape_data(base_urls['pitching_leaders'].format(year), f'college_pitching_leaders_{year}')\n",
    "        scrape_data(base_urls['fielding_leaders'].format(year), f'college_fielding_leaders_{year}')\n",
    "        scrape_data(base_urls['team_batting'].format(year), f'college_team_batting_{year}')\n",
    "        scrape_data(base_urls['team_pitching'].format(year), f'college_team_pitching_{year}')\n",
    "        scrape_data(base_urls['drafted_players'].format(year), f'college_drafted_players_{year}')\n",
    "\n",
    "# Run the scraper for 2015-2019 and 2021-2024, skipping 2020\n",
    "scrape_data_for_years(2015, 2024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another adjusted version that deals with the \"too many requests error\", basically adding in a delay between requests\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Base URLs\n",
    "base_urls = {\n",
    "    'batting_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=bat&year={}&group=College',\n",
    "    'pitching_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=pitch&year={}&group=College',\n",
    "    'fielding_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=field&year={}&group=College',\n",
    "    'team_batting': 'https://www.baseball-reference.com/register/team.cgi?request=1&type=bat&year={}&group=College',\n",
    "    'team_pitching': 'https://www.baseball-reference.com/register/team.cgi?request=1&type=pitch&year={}&group=College',\n",
    "    'drafted_players': 'https://www.baseball-reference.com/draft/?query_type=year_round&year_ID={}&draft_round=1&draft_type=college'\n",
    "}\n",
    "\n",
    "def scrape_data(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        \n",
    "        if not table:\n",
    "            print(f'No table found at {url}')\n",
    "            return\n",
    "        \n",
    "        headers = [th.get_text() for th in table.find_all('th')]\n",
    "        rows = []\n",
    "        for row in table.find_all('tr'):\n",
    "            rows.append([td.get_text() for td in row.find_all('td')])\n",
    "        \n",
    "        # Convert to DataFrame and save as CSV\n",
    "        df = pd.DataFrame(rows, columns=headers)\n",
    "        df.to_csv(f'{filename}.csv', index=False)\n",
    "        print(f'{filename}.csv saved.')\n",
    "        \n",
    "    except requests.HTTPError as http_err:\n",
    "        print(f'HTTP error occurred: {http_err}')\n",
    "    except Exception as err:\n",
    "        print(f'An error occurred: {err}')\n",
    "\n",
    "def scrape_data_for_years(start_year, end_year, delay=5):\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        if year == 2020:\n",
    "            print(f'Skipping the 2020 season...')\n",
    "            continue\n",
    "        \n",
    "        print(f'Scraping data for {year}...')\n",
    "        \n",
    "        scrape_data(base_urls['batting_leaders'].format(year), f'college_batting_leaders_{year}')\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        scrape_data(base_urls['pitching_leaders'].format(year), f'college_pitching_leaders_{year}')\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        scrape_data(base_urls['fielding_leaders'].format(year), f'college_fielding_leaders_{year}')\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        scrape_data(base_urls['team_batting'].format(year), f'college_team_batting_{year}')\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        scrape_data(base_urls['team_pitching'].format(year), f'college_team_pitching_{year}')\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        scrape_data(base_urls['drafted_players'].format(year), f'college_drafted_players_{year}')\n",
    "        time.sleep(delay)\n",
    "\n",
    "# Run the scraper for 2015-2019 and 2021-2024, with a 5-second delay between requests\n",
    "scrape_data_for_years(2015, 2024, delay=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Base URLs\n",
    "base_urls = {\n",
    "    'batting_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=bat&year={}&group=College',\n",
    "    'pitching_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=pitch&year={}&group=College',\n",
    "    'fielding_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=field&year={}&group=College',\n",
    "    'team_batting': 'https://www.baseball-reference.com/register/team.cgi?request=1&type=bat&year={}&group=College',\n",
    "    'team_pitching': 'https://www.baseball-reference.com/register/team.cgi?request=1&type=pitch&year={}&group=College',\n",
    "    'drafted_players': 'https://www.baseball-reference.com/draft/?query_type=year_round&year_ID={}&draft_round=1&draft_type=college'\n",
    "}\n",
    "\n",
    "# List of user agents to rotate\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\"\n",
    "]\n",
    "\n",
    "# Function to scrape data with retry logic and exponential backoff\n",
    "def scrape_data(url, filename, retries=5, backoff_factor=2):\n",
    "    attempt = 0\n",
    "    delay = 5\n",
    "    \n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            headers = {\n",
    "                \"User-Agent\": random.choice(user_agents)\n",
    "            }\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # If the response is successful, scrape the data\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            table = soup.find('table')\n",
    "\n",
    "            if not table:\n",
    "                print(f'No table found at {url}')\n",
    "                return\n",
    "            \n",
    "            headers = [th.get_text() for th in table.find_all('th')]\n",
    "            rows = []\n",
    "            for row in table.find_all('tr'):\n",
    "                rows.append([td.get_text() for td in row.find_all('td')])\n",
    "            \n",
    "            df = pd.DataFrame(rows, columns=headers)\n",
    "            df.to_csv(f'{filename}.csv', index=False)\n",
    "            print(f'{filename}.csv saved.')\n",
    "            break  # Break out of the loop if successful\n",
    "        \n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            if response.status_code == 429:\n",
    "                print(f'HTTP 429 error: Too many requests. Retrying in {delay} seconds...')\n",
    "                time.sleep(delay)\n",
    "                delay *= backoff_factor  # Exponential backoff\n",
    "                attempt += 1\n",
    "            else:\n",
    "                print(f'HTTP error occurred: {http_err}')\n",
    "                break\n",
    "        except Exception as err:\n",
    "            print(f'An error occurred: {err}')\n",
    "            break\n",
    "\n",
    "def scrape_data_for_years(start_year, end_year):\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        if year == 2020:\n",
    "            print(f'Skipping the 2020 season...')\n",
    "            continue\n",
    "        \n",
    "        print(f'Scraping data for {year}...')\n",
    "        \n",
    "        scrape_data(base_urls['batting_leaders'].format(year), f'college_batting_leaders_{year}')\n",
    "        scrape_data(base_urls['pitching_leaders'].format(year), f'college_pitching_leaders_{year}')\n",
    "        scrape_data(base_urls['fielding_leaders'].format(year), f'college_fielding_leaders_{year}')\n",
    "        scrape_data(base_urls['team_batting'].format(year), f'college_team_batting_{year}')\n",
    "        scrape_data(base_urls['team_pitching'].format(year), f'college_team_pitching_{year}')\n",
    "        scrape_data(base_urls['drafted_players'].format(year), f'college_drafted_players_{year}')\n",
    "\n",
    "# Run the scraper for 2015-2019 and 2021-2024\n",
    "scrape_data_for_years(2015, 2024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Base URLs\n",
    "base_urls = {\n",
    "    'batting_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=bat&year={}&group=College',\n",
    "    'pitching_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=pitch&year={}&group=College',\n",
    "    'fielding_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=field&year={}&group=College',\n",
    "    'team_batting': 'https://www.baseball-reference.com/register/team.cgi?request=1&type=bat&year={}&group=College',\n",
    "    'team_pitching': 'https://www.baseball-reference.com/register/team.cgi?request=1&type=pitch&year={}&group=College',\n",
    "    'drafted_players': 'https://www.baseball-reference.com/draft/?query_type=year_round&year_ID={}&draft_round=1&draft_type=college'\n",
    "}\n",
    "\n",
    "# List of user agents to rotate\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\"\n",
    "]\n",
    "\n",
    "# Function to scrape data with retry logic and exponential backoff\n",
    "def scrape_data(url, filename, retries=5, backoff_factor=2):\n",
    "    attempt = 0\n",
    "    delay = 5\n",
    "    \n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            headers = {\n",
    "                \"User-Agent\": random.choice(user_agents)\n",
    "            }\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # If the response is successful, scrape the data\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            table = soup.find('table')\n",
    "\n",
    "            if not table:\n",
    "                print(f'No table found at {url}')\n",
    "                return\n",
    "            \n",
    "            headers = [th.get_text() for th in table.find_all('th')]\n",
    "            rows = []\n",
    "            for row in table.find_all('tr'):\n",
    "                rows.append([td.get_text() for td in row.find_all('td')])\n",
    "            \n",
    "            df = pd.DataFrame(rows, columns=headers)\n",
    "            df.to_csv(f'{filename}.csv', index=False)\n",
    "            print(f'{filename}.csv saved.')\n",
    "            break  # Break out of the loop if successful\n",
    "        \n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            if response.status_code == 429:\n",
    "                print(f'HTTP 429 error: Too many requests. Retrying in {delay} seconds...')\n",
    "                time.sleep(delay)\n",
    "                delay *= backoff_factor  # Exponential backoff\n",
    "                attempt += 1\n",
    "            else:\n",
    "                print(f'HTTP error occurred: {http_err}')\n",
    "                break\n",
    "        except Exception as err:\n",
    "            print(f'An error occurred: {err}')\n",
    "            break\n",
    "\n",
    "def scrape_data_for_years(start_year, end_year):\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        if year == 2020:\n",
    "            print(f'Skipping the 2020 season...')\n",
    "            continue\n",
    "        \n",
    "        print(f'Scraping data for {year}...')\n",
    "        \n",
    "        scrape_data(base_urls['batting_leaders'].format(year), f'college_batting_leaders_{year}')\n",
    "        scrape_data(base_urls['pitching_leaders'].format(year), f'college_pitching_leaders_{year}')\n",
    "        scrape_data(base_urls['fielding_leaders'].format(year), f'college_fielding_leaders_{year}')\n",
    "        scrape_data(base_urls['team_batting'].format(year), f'college_team_batting_{year}')\n",
    "        scrape_data(base_urls['team_pitching'].format(year), f'college_team_pitching_{year}')\n",
    "        scrape_data(base_urls['drafted_players'].format(year), f'college_drafted_players_{year}')\n",
    "\n",
    "# Run the scraper for 2015-2019 and 2021-2024\n",
    "scrape_data_for_years(2015, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Base URLs\n",
    "base_urls = {\n",
    "    'batting_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=bat&year={}&group=College',\n",
    "    'pitching_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=pitch&year={}&group=College',\n",
    "    'fielding_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=field&year={}&group=College',\n",
    "    'team_batting': 'https://www.baseball-reference.com/register/team.cgi?request=1&type=bat&year={}&group=College',\n",
    "    'team_pitching': 'https://www.baseball-reference.com/register/team.cgi?request=1&type=pitch&year={}&group=College',\n",
    "    'drafted_players': 'https://www.baseball-reference.com/draft/?query_type=year_round&year_ID={}&draft_round=1&draft_type=college'\n",
    "}\n",
    "\n",
    "# List of user agents to rotate\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\"\n",
    "]\n",
    "\n",
    "# List of proxies to rotate\n",
    "proxies = [\n",
    "    \"http://proxy1.example.com:8080\",\n",
    "    \"http://proxy2.example.com:8080\",\n",
    "    \"http://proxy3.example.com:8080\"\n",
    "]\n",
    "\n",
    "# Function to scrape data with retry logic and exponential backoff\n",
    "def scrape_data(url, filename, retries=5, backoff_factor=2):\n",
    "    attempt = 0\n",
    "    delay = 5\n",
    "    \n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            headers = {\n",
    "                \"User-Agent\": random.choice(user_agents)\n",
    "            }\n",
    "            proxy = {\n",
    "                \"http\": random.choice(proxies),\n",
    "                \"https\": random.choice(proxies)\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, proxies=proxy)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # If the response is successful, scrape the data\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            table = soup.find('table')\n",
    "\n",
    "            if not table:\n",
    "                print(f'No table found at {url}')\n",
    "                return\n",
    "            \n",
    "            headers = [th.get_text() for th in table.find_all('th')]\n",
    "            rows = []\n",
    "            for row in table.find_all('tr'):\n",
    "                rows.append([td.get_text() for td in row.find_all('td')])\n",
    "            \n",
    "            df = pd.DataFrame(rows, columns=headers)\n",
    "            df.to_csv(f'{filename}.csv', index=False)\n",
    "            print(f'{filename}.csv saved.')\n",
    "            break  # Break out of the loop if successful\n",
    "        \n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            if response.status_code == 429:\n",
    "                print(f'HTTP 429 error: Too many requests. Retrying in {delay} seconds...')\n",
    "                time.sleep(delay)\n",
    "                delay *= backoff_factor  # Exponential backoff\n",
    "                attempt += 1\n",
    "            else:\n",
    "                print(f'HTTP error occurred: {http_err}')\n",
    "                break\n",
    "        except Exception as err:\n",
    "            print(f'An error occurred: {err}')\n",
    "            break\n",
    "\n",
    "def scrape_data_for_years(start_year, end_year):\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        if year == 2020:\n",
    "            print(f'Skipping the 2020 season...')\n",
    "            continue\n",
    "        \n",
    "        print(f'Scraping data for {year}...')\n",
    "        \n",
    "        scrape_data(base_urls['batting_leaders'].format(year), f'college_batting_leaders_{year}')\n",
    "        scrape_data(base_urls['pitching_leaders'].format(year), f'college_pitching_leaders_{year}')\n",
    "        scrape_data(base_urls['fielding_leaders'].format(year), f'college_fielding_leaders_{year}')\n",
    "        scrape_data(base_urls['team_batting'].format(year), f'college_team_batting_{year}')\n",
    "        scrape_data(base_urls['team_pitching'].format(year), f'college_team_pitching_{year}')\n",
    "        scrape_data(base_urls['drafted_players'].format(year), f'college_drafted_players_{year}')\n",
    "        \n",
    "        # Random delay between requests to avoid rate limiting\n",
    "        time.sleep(random.uniform(10, 30))\n",
    "\n",
    "# Run the scraper for 2015-2019 and 2021-2024\n",
    "scrape_data_for_years(2015, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for 2015...\n",
      "An error occurred: 130 columns passed, passed data had 29 columns\n",
      "An error occurred: 136 columns passed, passed data had 35 columns\n",
      "HTTP error occurred: 500 Server Error: Internal Server Error for url: https://www.baseball-reference.com/register/leader.cgi?request=1&type=field&year=2015&group=College\n",
      "No table found at https://www.baseball-reference.com/register/team.cgi?request=1&type=bat&year=2015&group=College\n",
      "No table found at https://www.baseball-reference.com/register/team.cgi?request=1&type=pitch&year=2015&group=College\n",
      "No table found at https://www.baseball-reference.com/draft/?query_type=year_round&year_ID=2015&draft_round=1&draft_type=college\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 88\u001b[0m\n\u001b[0;32m     85\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(random\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m30\u001b[39m))\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Run the scraper for 2015-2019 and 2021-2024\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m \u001b[43mscrape_data_for_years\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2015\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2024\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 85\u001b[0m, in \u001b[0;36mscrape_data_for_years\u001b[1;34m(start_year, end_year)\u001b[0m\n\u001b[0;32m     82\u001b[0m scrape_data(base_urls[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrafted_players\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mformat(year), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcollege_drafted_players_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Random delay between requests to avoid rate limiting\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Base URLs\n",
    "base_urls = {\n",
    "    'batting_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=bat&year={}&group=College',\n",
    "    'pitching_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=pitch&year={}&group=College',\n",
    "    'fielding_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=field&year={}&group=College',\n",
    "    'team_batting': 'https://www.baseball-reference.com/register/team.cgi?request=1&type=bat&year={}&group=College',\n",
    "    'team_pitching': 'https://www.baseball-reference.com/register/team.cgi?request=1&type=pitch&year={}&group=College',\n",
    "    'drafted_players': 'https://www.baseball-reference.com/draft/?query_type=year_round&year_ID={}&draft_round=1&draft_type=college'\n",
    "}\n",
    "\n",
    "# List of user agents to rotate\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\"\n",
    "]\n",
    "\n",
    "# Function to scrape data with retry logic and exponential backoff\n",
    "def scrape_data(url, filename, retries=5, backoff_factor=2):\n",
    "    attempt = 0\n",
    "    delay = 5\n",
    "    \n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            headers = {\n",
    "                \"User-Agent\": random.choice(user_agents)\n",
    "            }\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # If the response is successful, scrape the data\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            table = soup.find('table')\n",
    "\n",
    "            if not table:\n",
    "                print(f'No table found at {url}')\n",
    "                return\n",
    "            \n",
    "            headers = [th.get_text() for th in table.find_all('th')]\n",
    "            rows = []\n",
    "            for row in table.find_all('tr'):\n",
    "                rows.append([td.get_text() for td in row.find_all('td')])\n",
    "            \n",
    "            df = pd.DataFrame(rows, columns=headers)\n",
    "            df.to_csv(f'{filename}.csv', index=False)\n",
    "            print(f'{filename}.csv saved.')\n",
    "            break  # Break out of the loop if successful\n",
    "        \n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            if response.status_code == 429:\n",
    "                print(f'HTTP 429 error: Too many requests. Retrying in {delay} seconds...')\n",
    "                time.sleep(delay)\n",
    "                delay *= backoff_factor  # Exponential backoff\n",
    "                attempt += 1\n",
    "            else:\n",
    "                print(f'HTTP error occurred: {http_err}')\n",
    "                break\n",
    "        except Exception as err:\n",
    "            print(f'An error occurred: {err}')\n",
    "            break\n",
    "\n",
    "def scrape_data_for_years(start_year, end_year):\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        if year == 2020:\n",
    "            print(f'Skipping the 2020 season...')\n",
    "            continue\n",
    "        \n",
    "        print(f'Scraping data for {year}...')\n",
    "        \n",
    "        scrape_data(base_urls['batting_leaders'].format(year), f'college_batting_leaders_{year}')\n",
    "        scrape_data(base_urls['pitching_leaders'].format(year), f'college_pitching_leaders_{year}')\n",
    "        scrape_data(base_urls['fielding_leaders'].format(year), f'college_fielding_leaders_{year}')\n",
    "        scrape_data(base_urls['team_batting'].format(year), f'college_team_batting_{year}')\n",
    "        scrape_data(base_urls['team_pitching'].format(year), f'college_team_pitching_{year}')\n",
    "        scrape_data(base_urls['drafted_players'].format(year), f'college_drafted_players_{year}')\n",
    "        \n",
    "        # Random delay between requests to avoid rate limiting\n",
    "        time.sleep(random.uniform(10, 30))\n",
    "\n",
    "# Run the scraper for 2015-2019 and 2021-2024\n",
    "scrape_data_for_years(2015, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for 2015...\n",
      "college_batting_leaders_2015.csv saved.\n",
      "college_pitching_leaders_2015.csv saved.\n",
      "HTTP 500 error: Internal Server Error. Retrying in 10 seconds...\n",
      "HTTP 500 error: Internal Server Error. Retrying in 30 seconds...\n",
      "HTTP 500 error: Internal Server Error. Retrying in 90 seconds...\n",
      "HTTP 500 error: Internal Server Error. Retrying in 270 seconds...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Base URLs\n",
    "base_urls = {\n",
    "    'batting_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=bat&year={}&group=College',\n",
    "    'pitching_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=pitch&year={}&group=College',\n",
    "    'fielding_leaders': 'https://www.baseball-reference.com/register/leader.cgi?request=1&type=field&year={}&group=College',\n",
    "    'team_batting': 'https://www.baseball-reference.com/register/team.cgi?request=1&type=bat&year={}&group=College',\n",
    "    'team_pitching': 'https://www.baseball-reference.com/register/team.cgi?request=1&type=pitch&year={}&group=College',\n",
    "    'drafted_players': 'https://www.baseball-reference.com/draft/?query_type=year_round&year_ID={}&draft_round=1&draft_type=college'\n",
    "}\n",
    "\n",
    "# List of user agents to rotate\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\"\n",
    "]\n",
    "\n",
    "# Function to scrape data with retry logic and exponential backoff\n",
    "def scrape_data(url, filename, retries=5, backoff_factor=3):\n",
    "    attempt = 0\n",
    "    delay = 10  # Start with a longer initial delay\n",
    "    \n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            headers = {\n",
    "                \"User-Agent\": random.choice(user_agents)\n",
    "            }\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # If the response is successful, scrape the data\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            table = soup.find('table')\n",
    "\n",
    "            if not table:\n",
    "                print(f'No table found at {url}')\n",
    "                return\n",
    "            \n",
    "            headers = [th.get_text() for th in table.find_all('th')]\n",
    "            rows = []\n",
    "            for row in table.find_all('tr'):\n",
    "                rows.append([td.get_text() for td in row.find_all('td')])\n",
    "            \n",
    "            # Handle cases where the number of columns might not match\n",
    "            max_columns = max(len(row) for row in rows)\n",
    "            headers = headers[:max_columns]\n",
    "            rows = [row[:max_columns] for row in rows]\n",
    "            \n",
    "            df = pd.DataFrame(rows, columns=headers)\n",
    "            df.to_csv(f'{filename}.csv', index=False)\n",
    "            print(f'{filename}.csv saved.')\n",
    "            break  # Break out of the loop if successful\n",
    "        \n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            if response.status_code == 429:\n",
    "                print(f'HTTP 429 error: Too many requests. Retrying in {delay} seconds...')\n",
    "                time.sleep(delay)\n",
    "                delay *= backoff_factor  # Exponential backoff\n",
    "                attempt += 1\n",
    "            elif response.status_code == 500:\n",
    "                print(f'HTTP 500 error: Internal Server Error. Retrying in {delay} seconds...')\n",
    "                time.sleep(delay)\n",
    "                delay *= backoff_factor  # Exponential backoff\n",
    "                attempt += 1\n",
    "            else:\n",
    "                print(f'HTTP error occurred: {http_err}')\n",
    "                break\n",
    "        except Exception as err:\n",
    "            print(f'An error occurred: {err}')\n",
    "            break\n",
    "\n",
    "def scrape_data_for_years(start_year, end_year):\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        if year == 2020:\n",
    "            print(f'Skipping the 2020 season...')\n",
    "            continue\n",
    "        \n",
    "        print(f'Scraping data for {year}...')\n",
    "        \n",
    "        scrape_data(base_urls['batting_leaders'].format(year), f'college_batting_leaders_{year}')\n",
    "        scrape_data(base_urls['pitching_leaders'].format(year), f'college_pitching_leaders_{year}')\n",
    "        scrape_data(base_urls['fielding_leaders'].format(year), f'college_fielding_leaders_{year}')\n",
    "        scrape_data(base_urls['team_batting'].format(year), f'college_team_batting_{year}')\n",
    "        scrape_data(base_urls['team_pitching'].format(year), f'college_team_pitching_{year}')\n",
    "        scrape_data(base_urls['drafted_players'].format(year), f'college_drafted_players_{year}')\n",
    "        \n",
    "        # Random delay between requests to avoid rate limiting\n",
    "        time.sleep(random.uniform(30, 60))\n",
    "\n",
    "# Run the scraper for 2015-2019 and 2021-2024\n",
    "scrape_data_for_years(2015, 2024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
